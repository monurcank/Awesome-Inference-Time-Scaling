# Awesome-Inference-Time-Scaling
**Paper List of Inference/Test Time Scaling/Computing**

If you think our paper list is helpful, please Starâ­. Thanks! We will continue to update.

<!-- ![Scaling](./fig/Scaling.webp) -->
<p align="center">
    <img src="./fig/Scaling.webp" alt="Scaling" width="400">
</p>

Generated by DALLÂ·E.

## Plan
Collect papers and scale up paper list (Still Ongoing)

Categorize all papers for better reference (Next Step)

## How to Contribute? 

We understand that Inference/Test Time Scaling/Computing is a broad field. If you feel that our list is incomplete, **we warmly welcome your contributions (open a pull request or issue, I will handle it).**

If you can follow this format, that would be great.

First, you can fork my repo. Then, add the paper you think relevant. After that, open a pull request. Through my verification, I will accept and merge.

Below is the way you can use to run the code in my repo to call Semantic Scholar API to obtain the information and insert it to the README.

```
python fetch_semantic_info.py --paper_name "Paper Name or Key Word of the name"
```
Example:
```
python fetch_semantic_info.py --paper_name "Scaling Autonomous Agents via Automatic Reward Modeling And Planning"
```

If you find our code useful when you would like to organize your own repo, feel free to use. Also, thanks for the free use of [Semantic Scholar API](https://www.semanticscholar.org/product/api).


## ğŸ“– Paper List (Listed in Time Order)

ğŸ”¹ [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/pdf/2511.00640)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2511.00640)
- ğŸ‘¤ **Authors:** Zicheng Xu, Xiuyi Lou, Guanchu Wang, Yu-Neng Chuang, Feng Luo, Guangyao Zheng, Alexander S. Szalay, Zirui Liu, Vladimir Braverman
- ğŸ—“ï¸ **Date:** 2026-02-04
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>

    


ğŸ”¹ [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Youheng Zhu, Yiping Lu
- ğŸ—“ï¸ **Date:** 2026-02-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>
























ğŸ”¹ [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Zhengbo Jiao, Hongyu Xian, QING-LONG Wang, Yunpu Ma, Zhebo Wang, Zifan Zhang, Dezhang Kong, Meng Han
- ğŸ—“ï¸ **Date:** 2026-01-28
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>











ğŸ”¹ [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Qianyue Wang, Jinwu Hu, Yufeng Wang, Huanxiang Lin, Bolin Chen, Z. Wen, Yaofo Chen, Mingkui Tan
- ğŸ—“ï¸ **Date:** 2026-01-16
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>










ğŸ”¹ [Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Indranil Halder, Cengiz Pehlevan
- ğŸ—“ï¸ **Date:** 2025-12-22
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>







ğŸ”¹ [The Art of Scaling Test-Time Compute for Large Language Models](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty
- ğŸ—“ï¸ **Date:** 2025-12-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>

























ğŸ”¹ [Efficient Test-Time Scaling of Multi-Step Reasoning by Probing Internal States of Large Language Models](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Jingwei Ni, Ekaterina Fadeeva, Tianyi Wu, Mubashara Akhtar, Jiaheng Zhang, Elliott Ash, Markus Leippold, Timothy Baldwin, S. Ng, Artem Shelmanov, Mrinmaya Sachan
- ğŸ—“ï¸ **Date:** 2025-11-09
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>














ğŸ”¹ [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Zhuoyi Yang, Xu Guo, Tong Zhang, Huijuan Xu, Boyang Li
- ğŸ—“ï¸ **Date:** 2025-11-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>


























ğŸ”¹ [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Mehmet Onurcan Kaya, Desmond Elliott, Dim P. Papadopoulos
- ğŸ—“ï¸ **Date:** 2025-10-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>


ğŸ”¹ [SecInfer: Preventing Prompt Injection via Inference-time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Yupei Liu, Yanting Wang, Yuqi Jia, Jinyuan Jia, N. Gong
- ğŸ—“ï¸ **Date:** 2025-09-29
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>





ğŸ”¹ [DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Yinuo Ren, Wenhao Gao, Lexing Ying, Grant M. Rotskoff, Jiequn Han
- ğŸ—“ï¸ **Date:** 2025-09-25
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>


















ğŸ”¹ [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** James Xu Zhao, Bryan Hooi, S. Ng
- ğŸ—“ï¸ **Date:** 2025-09-08
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>








ğŸ”¹ [Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Shengyin Sun, Yiming Li, Xing Li, Yingzhao Lian, Weizhe Lin, Hui-Ling Zhen, Zhiyuan Yang, Chen Chen, Xianzhi Yu, Mingxuan Yuan, Chen Ma
- ğŸ—“ï¸ **Date:** 2025-08-30
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>





















ğŸ”¹ [FastTTS: Accelerating Test-Time Scaling for Edge LLM Reasoning](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** H. Chen, Zhiwen Mo, Guanxi Lu, Shuang Liang, Lingxiao Ma, Wayne Luk, Hongxiang Fan
- ğŸ—“ï¸ **Date:** 2025-08-29
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>






ğŸ”¹ [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Ramya Keerthy Thatikonda, W. Buntine, Ehsan Shareghi
- ğŸ—“ï¸ **Date:** 2025-08-27
- ğŸ“‘ **Publisher:** Conference on Empirical Methods in Natural Language Processing
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>



ğŸ”¹ [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Cong Liu, Wenchang Chai, Hejun Wu, Yan Pan, Pengxu Wei, Liang Lin
- ğŸ—“ï¸ **Date:** 2025-08-26
- ğŸ“‘ **Publisher:** Conference on Empirical Methods in Natural Language Processing
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>
















ğŸ”¹ [Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Hung-Chun Hsu, Yuan-Ching Kuo, C. Yang, Szu-Wei Fu, Hanrong Ye, Hongxu Yin, Yu-Chiang Frank Wang, Ming-Feng Tsai, Chuan-Ju Wang
- ğŸ—“ï¸ **Date:** 2025-08-25
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>















ğŸ”¹ [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, Yongliang Shen
- ğŸ—“ï¸ **Date:** 2025-08-07
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>





























ğŸ”¹ [CTTS: Collective Test-Time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Tao Chen
- ğŸ—“ï¸ **Date:** 2025-08-05
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>

















ğŸ”¹ [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, Junnan Li
- ğŸ—“ï¸ **Date:** 2025-07-08
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>




























ğŸ”¹ [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Youkang Wang, Jian Wang, Rubing Chen, Xiao-Yong Wei, Qing Li
- ğŸ—“ï¸ **Date:** 2025-06-27
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>




ğŸ”¹ [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2506.13585)
- ğŸ‘¤ **Authors:** MiniMax Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Ke Xiao, Le Han, Leyang Wang, Lian-Chun Yu, Li Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Ming-Yuan Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shu Yu, Sichen Li, S. Zhu, Tengfei Li, Tian-Yi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, X. Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yong Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yun-Wei Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Ze-Miao Li, Zhanxu Tian, Zhen-kun Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zi Gao, Zijia Wu, Zijian Song, Zijun Sun
- ğŸ—“ï¸ **Date:** 2025-06-16
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.
    </details>



































ğŸ”¹ [Strategic Scaling of Test-Time Compute: A Bandit Learning Approach](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Bowen Zuo, Yinglun Zhu
- ğŸ—“ï¸ **Date:** 2025-06-15
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>



























ğŸ”¹ [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2506.07976)
- ğŸ‘¤ **Authors:** Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Rajagopal Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, Aviral Kumar
- ğŸ—“ï¸ **Date:** 2025-06-09
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking"more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.
    </details>


































ğŸ”¹ [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/abs/2505.24863)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.24863)
- ğŸ‘¤ **Authors:** Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, Huan Zhang
- ğŸ—“ï¸ **Date:** 2025-05-30
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    This paper presents AlphaOne ($\alpha$1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. $\alpha$1 first introduces $\alpha$ moment, which represents the scaled thinking phase with a universal parameter $\alpha$. Within this scaled pre-$\alpha$ moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the $\alpha$ moment, $\alpha$1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate $\alpha$1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/
    </details>








































ğŸ”¹ [Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling](https://arxiv.org/abs/2506.15707)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2506.15707)
- ğŸ‘¤ **Authors:** Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li
- ğŸ—“ï¸ **Date:** 2025-05-30
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA's effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy. We hope our findings contribute to a broader understanding of optimal TTS for LLMs.
    </details>

































ğŸ”¹ [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/abs/2505.23621)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.23621)
- ğŸ‘¤ **Authors:** Zheyuan Yang, Lyuhao Chen, Arman Cohan, Yilun Zhao
- ğŸ—“ï¸ **Date:** 2025-05-29
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training.
    </details>







































ğŸ”¹ [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.20633)
- ğŸ‘¤ **Authors:** Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, Mingkui Tan
- ğŸ—“ï¸ **Date:** 2025-05-27
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.
    </details>













































ğŸ”¹ [Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.19621)
- ğŸ‘¤ **Authors:** George Kour, Itay Nakash, Ateret Anaby-Tavor, Michal Shmueli-Scheuer
- ğŸ—“ï¸ **Date:** 2025-05-26
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS
    </details>









































ğŸ”¹ [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.20325)
- ğŸ‘¤ **Authors:** Amirhosein Ghasemabadi, Keith G. Mills, Baochun Li, Di Niu
- ğŸ—“ï¸ **Date:** 2025-05-23
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques.
    </details>

















































ğŸ”¹ [Scaling Image and Video Generation via Test-Time Evolutionary Search](https://arxiv.org/abs/2505.17618)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.17618)
- ğŸ‘¤ **Authors:** Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Ling Pan
- ğŸ—“ï¸ **Date:** 2025-05-23
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose \textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.
    </details>














































ğŸ”¹ [Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling](https://arxiv.org/abs/2505.11730)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.11730)
- ğŸ‘¤ **Authors:** Hao Chen, Guanxi Lu, Yasuyuki Okoshi, Zhiwen Mo, Masato Motomura, Hongxiang Fan
- ğŸ—“ï¸ **Date:** 2025-05-16
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\% over Beam Search and 3.6\% over Best-of-N, while reducing FLOPs by over 52\%. We will open-source the code to support future research.
    </details>

















































ğŸ”¹ [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, Tieniu Tan
- ğŸ—“ï¸ **Date:** 2025-05-16
- ğŸ“‘ **Publisher:** Annual Meeting of the Association for Computational Linguistics
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>



















ğŸ”¹ [Crosslingual Reasoning through Test-Time Scaling](https://arxiv.org/abs/2505.05408)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.05408)
- ğŸ‘¤ **Authors:** Zheng-Xin Yong, M. Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, G. Winata, Julia Kreutzer, Stephen H. Bach, Alham Fikri Aji
- ğŸ—“ï¸ **Date:** 2025-05-08
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.
    </details>

















































ğŸ”¹ [Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers](https://arxiv.org/abs/2505.04842)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.04842)
- ğŸ‘¤ **Authors:** Kusha Sareen, Morgane M Moss, Alessandro Sordoni, Rishabh Agarwal, Arian Hosseini
- ğŸ—“ï¸ **Date:** 2025-05-07
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL$^V$ boosts MATH accuracy by over 20\% with parallel sampling and enables $8-32\times$ efficient test-time compute scaling compared to the base RL method. RL$^V$ also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves $1.2-1.6\times$ higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.
    </details>


















































ğŸ”¹ [Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2505.02686)
- ğŸ‘¤ **Authors:** Xiaobao Wu
- ğŸ—“ï¸ **Date:** 2025-05-05
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.
    </details>



















































ğŸ”¹ [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.16828)
- ğŸ‘¤ **Authors:** Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang
- ğŸ—“ï¸ **Date:** 2025-04-23
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
    </details>





















































ğŸ”¹ [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.16084)
- ğŸ‘¤ **Authors:** Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, Bowen Zhou
- ğŸ—“ï¸ **Date:** 2025-04-22
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
    </details>




















































ğŸ”¹ [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.13171)
- ğŸ‘¤ **Authors:** Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, Joseph Gonzalez
- ğŸ—“ï¸ **Date:** 2025-04-17
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to"think"offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.
    </details>























































ğŸ”¹ [M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models](https://arxiv.org/abs/2504.10449)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.10449)
- ğŸ‘¤ **Authors:** Junxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M. Rush, Tri Dao
- ğŸ—“ï¸ **Date:** 2025-04-14
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.
    </details>




























































ğŸ”¹ [Heimdall: test-time scaling on the generative verification](https://arxiv.org/abs/2504.10337)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.10337)
- ğŸ‘¤ **Authors:** Wenlei Shi, Xing Jin
- ğŸ—“ï¸ **Date:** 2025-04-14
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.
    </details>






















































ğŸ”¹ [VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search](https://arxiv.org/abs/2504.09130)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.09130)
- ğŸ‘¤ **Authors:** Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, D. Tao, Xipeng Qiu
- ğŸ—“ï¸ **Date:** 2025-04-12
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.
    </details>



























































ğŸ”¹ [SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning](https://arxiv.org/abs/2504.07891)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.07891)
- ğŸ‘¤ **Authors:** Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali
- ğŸ—“ï¸ **Date:** 2025-04-10
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to assess (and potentially correct) the speculated outputs. Importantly, SpecReason's focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of reasoning benchmarks, SpecReason achieves 1.5-2.5$\times$ speedup over vanilla LRM inference while improving accuracy by 1.0-9.9\%. Compared to speculative decoding without SpecReason, their combination yields an additional 19.4-44.2\% latency reduction. We open-source SpecReason at https://github.com/ruipeterpan/specreason.
    </details>


























































ğŸ”¹ [T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models](https://arxiv.org/abs/2504.04718)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.04718)
- ğŸ‘¤ **Authors:** Minki Kang, Jongwon Jeong, Jaewoong Cho
- ğŸ—“ï¸ **Date:** 2025-04-07
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.
    </details>





























































ğŸ”¹ [Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.02495)
- ğŸ‘¤ **Authors:** Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu
- ğŸ—“ï¸ **Date:** 2025-04-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.
    </details>





































































ğŸ”¹ [When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning](https://arxiv.org/abs/2504.01005)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.01005)
- ğŸ‘¤ **Authors:** Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach
- ğŸ—“ï¸ **Date:** 2025-04-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.
    </details>







































































ğŸ”¹ [GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning](https://arxiv.org/abs/2504.00891)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.00891)
- ğŸ‘¤ **Authors:** Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, Bowen Zhou
- ğŸ—“ï¸ **Date:** 2025-04-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.
    </details>




































































ğŸ”¹ [Z1: Efficient Test-time Scaling with Code](https://arxiv.org/abs/2504.00810)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.00810)
- ğŸ‘¤ **Authors:** Zhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, Xiao-Ping Zhang
- ğŸ—“ï¸ **Date:** 2025-04-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g.,. . .) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.
    </details>



































































ğŸ”¹ [m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models](https://arxiv.org/abs/2504.00869)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.00869)
- ğŸ‘¤ **Authors:** Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou
- ğŸ—“ï¸ **Date:** 2025-04-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.
    </details>


































































ğŸ”¹ [Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead](https://arxiv.org/abs/2504.00294)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2504.00294)
- ğŸ‘¤ **Authors:** Vidhisha Balachandran, Jingya Chen, Lingjiao Chen, Shivam Garg, Neel Joshi, Yash Lara, John Langford, Besmira Nushi, Vibhav Vineet, Yue Wu, Safoora Yousefi
- ğŸ—“ï¸ **Date:** 2025-03-31
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.
    </details>

































































ğŸ”¹ [What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2503.24235)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.24235)
- ğŸ‘¤ **Authors:** Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma
- ğŸ—“ï¸ **Date:** 2025-03-31
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.
    </details>
































































ğŸ”¹ [Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing](https://arxiv.org/abs/2503.19385)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.19385)
- ğŸ‘¤ **Authors:** Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung
- ğŸ—“ï¸ **Date:** 2025-03-25
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.
    </details>









































































ğŸ”¹ [Video-T1: Test-Time Scaling for Video Generation](https://arxiv.org/abs/2503.18942)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.18942)
- ğŸ‘¤ **Authors:** Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan
- ğŸ—“ï¸ **Date:** 2025-03-24
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos.
    </details>










































































ğŸ”¹ [VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](https://arxiv.org/abs/2503.13444)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.13444)
- ğŸ‘¤ **Authors:** Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou
- ğŸ—“ï¸ **Date:** 2025-03-17
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks, including 3 on grounded video question-answering (Grounded VideoQA), 6 on video temporal grounding (VTG), and 5 on general video question-answering (VideoQA), verify that our agent achieves state-of-the-art performance on diverse video understanding tasks, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.
    </details>






































































ğŸ”¹ [Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection](https://arxiv.org/abs/2503.12271)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.12271)
- ğŸ‘¤ **Authors:** Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, Aditya Grover
- ğŸ—“ï¸ **Date:** 2025-03-15
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.
    </details>






















































































ğŸ”¹ [VisualPRM: An Effective Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2503.10291)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.10291)
- ğŸ‘¤ **Authors:** Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong Duan, Yu Qiao, Jifeng Dai, Wenhai Wang
- ğŸ—“ï¸ **Date:** 2025-03-13
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the reasoning abilities of existing Multimodal Large Language Models (MLLMs) across different model scales and families with Best-of-N (BoN) evaluation strategies. Specifically, our model improves the reasoning performance of three types of MLLMs and four different model scales. Even when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. Experimental results show that our model exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation. To facilitate the training of multimodal PRMs, we construct a multimodal process supervision dataset VisualPRM400K using an automated data pipeline. For the evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with human-annotated step-wise correctness labels, to measure the abilities of PRMs to detect erroneous steps in multimodal reasoning tasks. We hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark are released in https://internvl.github.io/blog/2025-03-13-VisualPRM/.
    </details>
























































































ğŸ”¹ [Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Yuxiao Qu, Matthew Y. R. Yang, Amrith Rajagopal Setlur, Lewis Tunstall, E. Beeching, Ruslan Salakhutdinov, Aviral Kumar
- ğŸ—“ï¸ **Date:** 2025-03-10
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>























































































ğŸ”¹ [Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms](https://arxiv.org/abs/2503.07154)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.07154)
- ğŸ‘¤ **Authors:** Jiaming Song, Linqi Zhou
- ğŸ—“ï¸ **Date:** 2025-03-10
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency.
    </details>





















































































ğŸ”¹ [Language Models can Self-Improve at State-Value Estimation for Better Search](https://arxiv.org/abs/2503.02878)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.02878)
- ğŸ‘¤ **Authors:** Ethan Mendes, Alan Ritter
- ğŸ—“ï¸ **Date:** 2025-03-04
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages state-transition dynamics to train a value model capable of effectively guiding language model-controlled search. We find that moderately sized (8 billion parameters) open-weight value models improved with self-taught lookahead can match the performance of using a frontier LLM such as gpt-4o as the value model. Furthermore, we find that self-taught lookahead improves performance by 20% while reducing costs 37x compared to previous LLM-based tree search, without relying on ground truth rewards.
    </details>











































































ğŸ”¹ [LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/abs/2502.21321)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.21321)
- ğŸ‘¤ **Authors:** Komal Kumar, Tajamul Ashraf, Omkar Thawakar, R. Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, P. Torr, Salman H. Khan, Fahad Shahbaz Khan
- ğŸ—“ï¸ **Date:** 2025-02-28
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now increasingly shifting focus toward post-training techniques to achieve further breakthroughs. While pretraining provides a broad linguistic foundation, post-training methods enable LLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and align more effectively with user intents and ethical considerations. Fine-tuning, reinforcement learning, and test-time scaling have emerged as critical strategies for optimizing LLMs performance, ensuring robustness, and improving adaptability across various real-world tasks. This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. We highlight emerging directions in model alignment, scalable adaptation, and inference-time reasoning, and outline future research directions. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.
    </details>




























































































ğŸ”¹ [Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](https://arxiv.org/abs/2502.20339)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.20339)
- ğŸ‘¤ **Authors:** Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Y. Li, Aviv Bick, J. Kolter, Albert Gu, Franccois Fleuret, Tri Dao
- ğŸ—“ï¸ **Date:** 2025-02-27
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.
    </details>
































































































ğŸ”¹ [R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts](https://arxiv.org/abs/2502.20395)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.20395)
- ğŸ‘¤ **Authors:** Zhongyang Li, Ziyue Li, Tianyi Zhou
- ğŸ—“ï¸ **Date:** 2025-02-27
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method"Re-Routing in Test-Time (R2-T2)"that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.
    </details>






























































ğŸ”¹ [TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Henry Peng Zou, Zhengyao Gu, Yue Zhou, Yankai Chen, Weizhi Zhang, Liancheng Fang, Yibo Wang, Yangning Li, Kay Liu, Philip S. Yu
- ğŸ—“ï¸ **Date:** 2025-02-26
- ğŸ“‘ **Publisher:** Annual Meeting of the Association for Computational Linguistics
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>






























ğŸ”¹ [Efficient Test-Time Scaling via Self-Calibration](https://arxiv.org/abs/2503.00031)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2503.00031)
- ğŸ‘¤ **Authors:** Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, Jiaxin Huang
- ğŸ—“ï¸ **Date:** 2025-02-25
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time.
    </details>





























































































ğŸ”¹ [Rank1: Test-Time Compute for Reranking in Information Retrieval](https://arxiv.org/abs/2502.18418)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.18418)
- ğŸ‘¤ **Authors:** Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, Benjamin Van Durme
- ğŸ—“ï¸ **Date:** 2025-02-25
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search.
    </details>































































ğŸ”¹ [METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling](https://arxiv.org/abs/2502.17651)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.17651)
- ğŸ‘¤ **Authors:** Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, Nanyun Peng
- ğŸ—“ï¸ **Date:** 2025-02-24
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.
    </details>

































































































ğŸ”¹ [From System 1 to System 2: A Survey of Reasoning Large Language Models](https://arxiv.org/abs/2502.17419)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.17419)
- ğŸ‘¤ **Authors:** Zhongzhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu
- ğŸ—“ï¸ **Date:** 2025-02-24
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.
    </details>































































































ğŸ”¹ [S*: Test Time Scaling for Code Generation](https://arxiv.org/abs/2502.14382)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.14382)
- ğŸ‘¤ **Authors:** Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph Gonzalez, Ion Stoica
- ğŸ—“ï¸ **Date:** 2025-02-20
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought.
    </details>



































































































ğŸ”¹ [Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?](https://arxiv.org/abs/2502.12215)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.12215)
- ğŸ‘¤ **Authors:** Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu
- ğŸ—“ï¸ **Date:** 2025-02-17
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.
    </details>






































































































ğŸ”¹ [Scaling Autonomous Agents via Automatic Reward Modeling And Planning](https://arxiv.org/abs/2502.12130)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.12130)
- ğŸ‘¤ **Authors:** Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan
- ğŸ—“ï¸ **Date:** 2025-02-17
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.
    </details>





































































































ğŸ”¹ [Scaling Test-Time Compute Without Verification or RL is Suboptimal](https://arxiv.org/abs/2502.12118)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.12118)
- ğŸ‘¤ **Authors:** Amrith Rajagopal Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar
- ğŸ—“ï¸ **Date:** 2025-02-17
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: first, distilling successful search or thinking traces; and second, using verification (e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erd\H{o}s, 1945]. This implies a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF methods widening as test-time budget grows. We corroborate our theory empirically on both didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.
    </details>




































































































ğŸ”¹ [Atom of Thoughts for Markov LLM Test-Time Scaling](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo, Zhijiang Guo
- ğŸ—“ï¸ **Date:** 2025-02-17
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>























ğŸ”¹ [Diverse Inference and Verification for Advanced Reasoning](https://arxiv.org/abs/2502.09955)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.09955)
- ğŸ‘¤ **Authors:** Iddo Drori, Gaston Longhitano, Mao Mao, Seunghwan Hyun, Yuke Zhang, Sungjun Park, Zachary Meeks, Xin-Yu Zhang, Ben Segev, Howard Yong, Nakul Verma, A. Shporer, Alon Amit, Madeleine Udell
- ğŸ—“ï¸ **Date:** 2025-02-14
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.
    </details>







































































































ğŸ”¹ [Monte Carlo Tree Diffusion for System 2 Planning](https://arxiv.org/abs/2502.07202)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.07202)
- ğŸ‘¤ **Authors:** Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Y. Bengio, Sungjin Ahn
- ğŸ—“ï¸ **Date:** 2025-02-11
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with additional test-time computation (TTC), standard diffusion-based planners offer only limited avenues for TTC scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as TTC increases.
    </details>









































































































ğŸ”¹ [Bag of Tricks for Inference-time Computation of LLM Reasoning](https://arxiv.org/abs/2502.07191)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.07191)
- ğŸ‘¤ **Authors:** Fan Liu, WenShuo Chao, Naiqiang Tan, Hao Liu
- ğŸ—“ï¸ **Date:** 2025-02-11
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at https://github.com/usail-hkust/benchmark_inference_time_computation_LLM
    </details>

























































































ğŸ”¹ [Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling](https://arxiv.org/abs/2502.06703)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.06703)
- ğŸ‘¤ **Authors:** Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou
- ğŸ—“ï¸ **Date:** 2025-02-10
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.
    </details>










































































































ğŸ”¹ [Generating Symbolic World Models via Test-time Scaling of Large Language Models](https://arxiv.org/abs/2502.04728)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.04728)
- ğŸ‘¤ **Authors:** Zhouliang Yu, Yuhuan Yuan, Tim Z. Xiao, Fuxiang Frank Xia, Jie Fu, Ge Zhang, Ge Lin, Weiyang Liu
- ğŸ—“ï¸ **Date:** 2025-02-07
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.
    </details>












































































































ğŸ”¹ [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.05171)
- ğŸ‘¤ **Authors:** Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein
- ğŸ—“ï¸ **Date:** 2025-02-07
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.
    </details>











































































































ğŸ”¹ [Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis](https://arxiv.org/abs/2502.04128)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.04128)
- ğŸ‘¤ **Authors:** Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue
- ğŸ—“ï¸ **Date:** 2025-02-06
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.
    </details>

















































































































ğŸ”¹ [Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking](https://arxiv.org/abs/2502.02339)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.02339)
- ğŸ‘¤ **Authors:** Jinyang Wu, Mingkuan Feng, Shuai Zhang, Ruihan Jin, Feihu Che, Zengqi Wen, Jianhua Tao
- ğŸ—“ï¸ **Date:** 2025-02-04
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\%$) while maintaining substantial data and computational efficiency.
    </details>
































































































































ğŸ”¹ [QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search](https://arxiv.org/abs/2502.02584)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.02584)
- ğŸ‘¤ **Authors:** Zongyu Lin, Yao Tang, Xingcheng Yao, Da Yin, Ziniu Hu, Yizhou Sun, Kai-Wei Chang
- ğŸ—“ï¸ **Date:** 2025-02-04
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.
    </details>















































































































ğŸ”¹ [ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning](https://arxiv.org/abs/2502.01100)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.01100)
- ğŸ‘¤ **Authors:** Bill Yuchen Lin, R. L. Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, Yejin Choi
- ğŸ—“ï¸ **Date:** 2025-02-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty. Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.
    </details>

































































































































ğŸ”¹ [Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification](https://arxiv.org/abs/2502.01839)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.01839)
- ğŸ‘¤ **Authors:** Eric Zhao, Pranjal Awasthi, Sreenivas Gollapudi
- ğŸ—“ï¸ **Date:** 2025-02-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.
    </details>
















































































































ğŸ”¹ [Almost Surely Safe Alignment of Large Language Models at Inference-Time](https://arxiv.org/abs/2502.01208)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2502.01208)
- ğŸ‘¤ **Authors:** Xiaotong Ji, Shyam Sundhar Ramesh, Matthieu Zimmer, Ilija Bogunovic, Jun Wang, H. Ammar
- ğŸ—“ï¸ **Date:** 2025-02-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.
    </details>














































































































ğŸ”¹ [Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava
- ğŸ—“ï¸ **Date:** 2025-02-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>












ğŸ”¹ [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.19393)
- ğŸ‘¤ **Authors:** Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Fei-Fei Li, Hanna Hajishirzi, Luke S. Zettlemoyer, Percy Liang, Emmanuel J. Candes, Tatsunori Hashimoto
- ğŸ—“ï¸ **Date:** 2025-01-31
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending"Wait"multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.
    </details>




































































































































ğŸ”¹ [Trading Inference-Time Compute for Adversarial Robustness](https://arxiv.org/abs/2501.18841)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.18841)
- ğŸ‘¤ **Authors:** Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Jo-hannes Heidecke, Amelia Glaese
- ğŸ—“ï¸ **Date:** 2025-01-31
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.
    </details>



































































































































ğŸ”¹ [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/abs/2501.19306)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.19306)
- ğŸ‘¤ **Authors:** Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan Ã–. Arik
- ğŸ—“ï¸ **Date:** 2025-01-31
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws.
    </details>


































































































































ğŸ”¹ [SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer](https://arxiv.org/abs/2501.18427)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.18427)
- ğŸ‘¤ **Authors:** Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, Song Han
- ğŸ—“ï¸ **Date:** 2025-01-30
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    This paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer. (2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss. (3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time. Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.72 on GenEval, which can be further improved to 0.80 through inference scaling, establishing a new SoTA on GenEval benchmark. These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible.
    </details>

















































































































































ğŸ”¹ [CodeMonkeys: Scaling Test-Time Compute for Software Engineering](https://arxiv.org/abs/2501.14723)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.14723)
- ğŸ‘¤ **Authors:** Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher R'e, Azalia Mirhoseini
- ğŸ—“ï¸ **Date:** 2025-01-24
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Scaling test-time compute is a promising axis for improving LLM capabilities. However, test-time compute can be scaled in a variety of ways, and effectively combining different approaches remains an active area of research. Here, we explore this problem in the context of solving real-world GitHub issues from the SWE-bench dataset. Our system, named CodeMonkeys, allows models to iteratively edit a codebase by jointly generating and running a testing script alongside their draft edit. We sample many of these multi-turn trajectories for every issue to generate a collection of candidate edits. This approach lets us scale"serial"test-time compute by increasing the number of iterations per trajectory and"parallel"test-time compute by increasing the number of trajectories per problem. With parallel scaling, we can amortize up-front costs across multiple downstream samples, allowing us to identify relevant codebase context using the simple method of letting an LLM read every file. In order to select between candidate edits, we combine voting using model-generated tests with a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys resolves 57.4% of issues from SWE-bench Verified using a budget of approximately 2300 USD. Our selection method can also be used to combine candidates from different sources. Selecting over an ensemble of edits from existing top SWE-bench Verified submissions obtains a score of 66.2% and outperforms the best member of the ensemble on its own. We fully release our code and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.
    </details>

















































































































































ğŸ”¹ [Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps](https://arxiv.org/abs/2501.09732)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.09732)
- ğŸ‘¤ **Authors:** Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, T. Jaakkola, Xuhui Jia, Saining Xie
- ğŸ—“ï¸ **Date:** 2025-01-16
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.
    </details>

















































































































































ğŸ”¹ [A General Framework for Inference-time Scaling and Steering of Diffusion Models](https://arxiv.org/abs/2501.06848)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.06848)
- ğŸ‘¤ **Authors:** Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
- ğŸ—“ï¸ **Date:** 2025-01-12
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Diffusion models produce impressive results in modalities ranging from images and video to protein design and text. However, generating samples with user-specified properties remains a challenge. Recent research proposes fine-tuning models to maximize rewards that capture desired properties, but these methods require expensive training and are prone to mode collapse. In this work, we propose Feynman Kac (FK) steering, an inference-time framework for steering diffusion models with reward functions. FK steering works by sampling a system of multiple interacting diffusion processes, called particles, and resampling particles at intermediate steps based on scores computed using functions called potentials. Potentials are defined using rewards for intermediate states and are selected such that a high value indicates that the particle will yield a high-reward sample. We explore various choices of potentials, intermediate rewards, and samplers. We evaluate FK steering on text-to-image and text diffusion models. For steering text-to-image models with a human preference reward, we find that FK steering a 0.8B parameter model outperforms a 2.6B parameter fine-tuned model on prompt fidelity, with faster sampling and no training. For steering text diffusion models with rewards for text quality and specific text attributes, we find that FK steering generates lower perplexity, more linguistically acceptable outputs and enables gradient-free control of attributes like toxicity. Our results demonstrate that inference-time scaling and steering of diffusion models, even with off-the-shelf rewards, can provide significant sample quality gains and controllability benefits. Code is available at https://github.com/zacharyhorvitz/Fk-Diffusion-Steering.
    </details>

















































































































































ğŸ”¹ [O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning](https://arxiv.org/abs/2501.06458)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.06458)
- ğŸ‘¤ **Authors:** Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang
- ğŸ—“ï¸ **Date:** 2025-01-11
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Building upon our previous investigations of O1 replication (Part 1: Journey Learning [Qin et al., 2024] and Part 2: Distillation [Huang et al., 2024]), this work explores the potential of inference-time scaling in large language models (LLMs) for medical reasoning tasks, ranging from diagnostic decision-making to treatment planning. Through extensive experiments on medical benchmarks of varying complexity (MedQA, Medbullets, and JAMA Clinical Challenges), our investigation reveals several key insights: (1) Increasing inference time does lead to improved performance. With a modest training set of 500 samples, our model yields substantial performance improvements of 6%-11%. (2) Task complexity directly correlates with the required length of reasoning chains, confirming the necessity of extended thought processes for challenging problems. (3) The differential diagnoses generated by our model adhere to the principles of the hypothetico-deductive method, producing a list of potential conditions that may explain a patient's symptoms and systematically narrowing these possibilities by evaluating the evidence. These findings demonstrate the promising synergy between inference-time scaling and journey learning in advancing LLMs' real-world clinical reasoning capabilities.
    </details>

















































































































































ğŸ”¹ [rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://arxiv.org/abs/2501.04519)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.04519)
- ğŸ‘¤ **Authors:** Xinyu Guan, L. Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang
- ğŸ—“ï¸ **Date:** 2025-01-08
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising"deep thinking"through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.
    </details>









































































































































ğŸ”¹ [Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though](https://arxiv.org/abs/2501.04682)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.04682)
- ğŸ‘¤ **Authors:** Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, nathan lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, Chelsea Finn
- ğŸ—“ï¸ **Date:** 2025-01-08
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.
    </details>








































































































































ğŸ”¹ [Test-time Computing: from System-1 Thinking to System-2 Thinking](https://arxiv.org/abs/2501.02497)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2501.02497)
- ğŸ‘¤ **Authors:** Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, Min Zhang
- ğŸ—“ï¸ **Date:** 2025-01-05
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.
    </details>































































































































ğŸ”¹ [Efficiently Scaling LLM Reasoning with Certaindex](https://arxiv.org/abs/N/A)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/N/A)
- ğŸ‘¤ **Authors:** Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Yonghao Zhuang, Yian Ma, Aurick Qiao, Tajana Rosing, Ion Stoica, Hao Zhang
- ğŸ—“ï¸ **Date:** 2024-12-30
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    No abstract available.
    </details>









ğŸ”¹ [Outcome-Refining Process Supervision for Code Generation](https://arxiv.org/abs/2412.15118)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2412.15118)
- ğŸ‘¤ **Authors:** Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shi-Bo Zhang
- ğŸ—“ï¸ **Date:** 2024-12-19
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS
    </details>




















































































ğŸ”¹ [Proposing and solving olympiad geometry with guided tree search](https://arxiv.org/abs/2412.10673)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2412.10673)
- ğŸ‘¤ **Authors:** Chi Zhang, Jiajun Song, Siyu Li, Yitao Liang, Yuxi Ma, Wei Wang, Yixin Zhu, Song-Chun Zhu
- ğŸ—“ï¸ **Date:** 2024-12-14
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Mathematics olympiads are prestigious competitions, with problem proposing and solving highly honored. Building artificial intelligence that proposes and solves olympiads presents an unresolved challenge in automated theorem discovery and proving, especially in geometry for its combination of numerical and spatial elements. We introduce TongGeometry, a Euclidean geometry system supporting tree-search-based guided problem proposing and solving. The efficient geometry system establishes the most extensive repository of geometry theorems to date: within the same computational budget as the existing state-of-the-art, TongGeometry discovers 6.7 billion geometry theorems requiring auxiliary constructions, including 4.1 billion exhibiting geometric symmetry. Among them, 10 theorems were proposed to regional mathematical olympiads with 3 of TongGeometry's proposals selected in real competitions, earning spots in a national team qualifying exam or a top civil olympiad in China and the US. Guided by fine-tuned large language models, TongGeometry solved all International Mathematical Olympiad geometry in IMO-AG-30, outperforming gold medalists for the first time. It also surpasses the existing state-of-the-art across a broader spectrum of olympiad-level problems. The full capabilities of the system can be utilized on a consumer-grade machine, making the model more accessible and fostering widespread democratization of its use. By analogy, unlike existing systems that merely solve problems like students, TongGeometry acts like a geometry coach, discovering, presenting, and proving theorems.
    </details>


















































































ğŸ”¹ [Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning](https://arxiv.org/abs/2412.09078)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2412.09078)
- ğŸ‘¤ **Authors:** Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang
- ğŸ—“ï¸ **Date:** 2024-12-12
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large Language Models (LLMs) have demonstrated remarkable abilities across various language tasks, but solving complex reasoning problems remains a significant challenge. While existing methods, such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this limitation, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT employs sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction, along with consensus-guided decision-making strategies to optimize both correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency.Code will be available at https://github.com/iamhankai/Forest-of-Thought.
    </details>



















































































ğŸ”¹ [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2412.05271)
- ğŸ‘¤ **Authors:** Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Hui Deng, Jiaye Ge, Kaiming Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahu Lin, Yunfeng Qiao, Jifeng Dai, Wenhai Wang
- ğŸ—“ï¸ **Date:** 2024-12-06
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL
    </details>






























































































ğŸ”¹ [LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs in Seconds](https://arxiv.org/abs/2412.05232)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2412.05232)
- ğŸ‘¤ **Authors:** James Beetham, Souradip Chakraborty, Mengdi Wang, Furong Huang, A. S. Bedi, Mubarak Shah
- ğŸ—“ï¸ **Date:** 2024-12-06
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Traditional jailbreaks have successfully exposed vulnerabilities in LLMs, primarily relying on discrete combinatorial optimization, while more recent methods focus on training LLMs to generate adversarial prompts. However, both approaches are computationally expensive and slow, often requiring significant resources to generate a single successful attack. We hypothesize that the inefficiency of these methods arises from an inadequate characterization of the jailbreak problem itself. To address this gap, we approach the jailbreak problem as an alignment problem, leading us to propose LIAR (Leveraging Inference time Alignment to jailbReak), a fast and efficient best-of-N approach tailored for jailbreak attacks. LIAR offers several key advantages: it eliminates the need for additional training, operates in a fully black-box setting, significantly reduces computational overhead, and produces more human-readable adversarial prompts while maintaining competitive attack success rates. Our results demonstrate that a best-of-N approach is a simple yet highly effective strategy for evaluating the robustness of aligned LLMs, achieving attack success rates (ASR) comparable to state-of-the-art methods while offering a 10x improvement in perplexity and a significant speedup in Time-to-Attack, reducing execution time from tens of hours to seconds. Additionally, We also provide sub-optimality guarantees for the proposed LIAR. Our work highlights the potential of efficient, alignment-based jailbreak strategies for assessing and stress-testing AI safety measures.
    </details>


























































































ğŸ”¹ [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/abs/2412.03704)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2412.03704)
- ğŸ‘¤ **Authors:** Wang Xiyao, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Lin Chung-Ching Lin, Lin Kevin, Furong Huang, Lijuan Wang
- ğŸ—“ï¸ **Date:** 2024-12-04
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM.
    </details>










































































































































ğŸ”¹ [A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.19477)
- ğŸ‘¤ **Authors:** Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou
- ğŸ—“ï¸ **Date:** 2024-11-29
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $N$ candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for $K$ times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of $N \times (K + 1)$ highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability $p_{\text{gen}}>0$ and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability $p_{\text{comp}}>0.5$ (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to $N$ and $K$: $$\mathbb{P}(\text{final output is incorrect}) \le (1 - p_{\text{gen}})^N + \lceil \log_2 N \rceil e^{-2 K (p_{\text{comp}} - 0.5)^2}.$$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.
    </details>





































































































































ğŸ”¹ [o1-Coder: an o1 Replication for Coding](https://arxiv.org/abs/2412.00154)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2412.00154)
- ğŸ‘¤ **Authors:** Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang
- ğŸ—“ï¸ **Date:** 2024-11-29
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode and then generate the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for world model construction. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models are disclosed at https://github.com/ADaM-BJTU/O1-CODER .
    </details>





























































































































ğŸ”¹ [Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS](https://arxiv.org/abs/2411.18478)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.18478)
- ğŸ‘¤ **Authors:** Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, Jianhua Tao
- ğŸ—“ï¸ **Date:** 2024-11-27
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a \textbf{Hi}gh-level \textbf{A}utomated \textbf{R}easoning paradigm in \textbf{ICL} that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6$\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6$\%$) and Claude 3.5 (71.1$\%$).
    </details>























































































































ğŸ”¹ [Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/abs/2411.14405)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.14405)
- ğŸ‘¤ **Authors:** Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang
- ğŸ—“ï¸ **Date:** 2024-11-21
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Currently OpenAI o1 sparks a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding -- which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: ''Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?'' Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks.
    </details>
























































































































ğŸ”¹ [Enhancing LLM Reasoning with Reward-guided Tree Search](https://arxiv.org/abs/2411.11694)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.11694)
- ğŸ‘¤ **Authors:** Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, Zheng Liu, Dong Yan, Jian Xie, Zhongyuan Wang, Jiahui Wen
- ğŸ—“ï¸ **Date:** 2024-11-18
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recently, test-time scaling has garnered significant attention from the research community, largely due to the substantial advancements of the o1 model released by OpenAI. By allocating more computational resources during the inference phase, large language models~(LLMs) can extensively explore the solution space by generating more thought tokens or diverse solutions, thereby producing more accurate responses. However, developing an o1-like reasoning approach is challenging, and researchers have been making various attempts to advance this open area of research. In this paper, we present a preliminary exploration into enhancing the reasoning abilities of LLMs through reward-guided tree search algorithms. This framework is implemented by integrating the policy model, reward model, and search algorithm. It is primarily constructed around a tree search algorithm, where the policy model navigates a dynamically expanding tree guided by a specially trained reward model. The implemented framework is denoted as \textbf{STILL-1}. We thoroughly explore various design considerations necessary for implementing this framework and provide a detailed report of the technical aspects. To assess the effectiveness of our approach, we focus on mathematical reasoning tasks and conduct extensive evaluations on four challenging datasets, significantly enhancing the reasoning abilities of LLMs.
    </details>








































































































ğŸ”¹ [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.10440)
- ğŸ‘¤ **Authors:** Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan
- ğŸ—“ï¸ **Date:** 2024-11-15
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-CoT not only outperforms its base model by 7.4% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.
    </details>












































































































































ğŸ”¹ [The Surprising Effectiveness of Test-Time Training for Abstract Reasoning](https://arxiv.org/abs/2411.07279)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.07279)
- ğŸ‘¤ **Authors:** Ekin AkyÃ¼rek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, Jacob Andreas
- ğŸ—“ï¸ **Date:** 2024-11-11
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. We investigate the effectiveness of test-time training (TTT) -- updating model parameters temporarily during inference using a loss derived from input data -- as a mechanism for improving models' reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, we identify three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6x improvement in accuracy compared to base fine-tuned models; applying TTT to an 8B-parameter language model, we achieve 53% accuracy on the ARC's public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches. By ensembling our method with recent program generation approaches, we get SoTA public validation accuracy of 61.9%, matching the average human score. Our findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective.
    </details>

























































































































ğŸ”¹ [Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents](https://arxiv.org/abs/2411.06559)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.06559)
- ğŸ‘¤ **Authors:** Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, Yu Su
- ğŸ—“ï¸ **Date:** 2024-11-10
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Language agents have demonstrated promising capabilities in automating web-based tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents' performance, implementing tree search directly on live websites poses significant safety risks and practical constraints due to irreversible actions such as confirming a purchase. In this paper, we introduce a novel paradigm that augments language agents with model-based planning, pioneering the innovative use of large language models (LLMs) as world models in complex web environments. Our method, WebDreamer, builds on the key insight that LLMs inherently encode comprehensive knowledge about website structures and functionalities. Specifically, WebDreamer uses LLMs to simulate outcomes for each candidate action (e.g.,"what would happen if I click this button?") using natural language descriptions, and then evaluates these imagined outcomes to determine the optimal action at each step. Empirical results on two representative web agent benchmarks with online interaction -- VisualWebArena and Mind2Web-live -- demonstrate that WebDreamer achieves substantial improvements over reactive baselines. By establishing the viability of LLMs as world models in web environments, this work lays the groundwork for a paradigm shift in automated web interaction. More broadly, our findings open exciting new avenues for future research into 1) optimizing LLMs specifically for world modeling in complex, dynamic environments, and 2) model-based speculative planning for language agents.
    </details>














































































































































ğŸ”¹ [CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models](https://arxiv.org/abs/2411.04329)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.04329)
- ğŸ‘¤ **Authors:** Jierui Li, Hung Le, Yinbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo
- ğŸ—“ï¸ **Date:** 2024-11-07
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0 on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains.
    </details>

















































































ğŸ”¹ [GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial Fraud Detection](https://arxiv.org/abs/2411.04459)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2411.04459)
- ğŸ‘¤ **Authors:** Prashank Kadam
- ğŸ—“ï¸ **Date:** 2024-11-07
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    With the increasing number of financial services available online, the rate of financial fraud has also been increasing. The traffic and transaction rates on the internet have increased considerably, leading to a need for fast decision-making. Financial institutions also have stringent regulations that often require transparency and explainability of the decision-making process. However, most state-of-the-art algorithms currently used in the industry are highly parameterized black-box models that rely on complex computations to generate a score. These algorithms are inherently slow and lack the explainability and speed of traditional rule-based learners. This work introduces SR-MCTS (Symbolic Regression MCTS), which utilizes a foundational GPT model to guide the MCTS, significantly enhancing its convergence speed and the quality of the generated expressions which are further extracted to rules. Our experiments show that SR-MCTS can detect fraud more efficiently than widely used methods in the industry while providing substantial insights into the decision-making process.
    </details>
















































































ğŸ”¹ [Non-myopic Generation of Language Models for Reasoning and Planning](https://arxiv.org/abs/2410.17195)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2410.17195)
- ğŸ‘¤ **Authors:** Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong
- ğŸ—“ï¸ **Date:** 2024-10-22
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large Language Models have demonstrated remarkable abilities in reasoning and planning by breaking down complex problems into sequential steps. Despite their success in various domains like mathematical problem-solving and coding, LLMs face challenges in ensuring reliable and optimal planning due to their inherent myopic nature of autoregressive decoding. This paper revisits LLM reasoning from an optimal-control perspective, proposing a novel method, Predictive-Decoding, that leverages Model Predictive Control to enhance planning accuracy. By re-weighting LLM distributions based on foresight trajectories, Predictive-Decoding aims to mitigate early errors and promote non-myopic planning. Our experiments show significant improvements in a wide range of tasks for math, coding, and agents. Furthermore, Predictive-Decoding demonstrates computational efficiency, outperforming search baselines with reduced computational resources. This study provides insights into optimizing LLM planning capabilities.
    </details>


































































































ğŸ”¹ [Improve Vision Language Model Chain-of-thought Reasoning](https://arxiv.org/abs/2410.16198)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2410.16198)
- ğŸ‘¤ **Authors:** Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, Yiming Yang
- ğŸ—“ï¸ **Date:** 2024-10-21
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.
    </details>







































































































































ğŸ”¹ [TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling](https://arxiv.org/abs/2410.16033)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2410.16033)
- ğŸ‘¤ **Authors:** Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, Mengdi Wang
- ğŸ—“ï¸ **Date:** 2024-10-18
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Inference-time alignment enhances the performance of large language models without requiring additional training or fine-tuning but presents challenges due to balancing computational efficiency with high-quality output. Best-of-N (BoN) sampling, as a simple yet powerful approach, generates multiple responses and selects the best one, achieving improved performance but with a high computational cost. We propose TreeBoN, a novel framework that integrates a speculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN maintains a set of parent nodes, iteratively branching and pruning low-quality responses, thereby reducing computational overhead while maintaining high output quality. Our approach also leverages token-level rewards from Direct Preference Optimization (DPO) to guide tree expansion and prune low-quality paths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and TutorEval datasets, demonstrating consistent improvements. Specifically, TreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win rates across other different datasets, outperforming standard BoN with the same computational cost and showcasing its scalability and alignment efficacy.
    </details>





















































































































ğŸ”¹ [Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs](https://arxiv.org/abs/2410.08020)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2410.08020)
- ğŸ‘¤ **Authors:** Jonas HÃ¼botter, Sascha Bongni, Ido Hakimi, Andreas Krause
- ğŸ—“ï¸ **Date:** 2024-10-10
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent efforts in fine-tuning language models often rely on automatic data selection, commonly using Nearest Neighbors retrieval from large datasets. However, we theoretically show that this approach tends to select redundant data, limiting its effectiveness or even hurting performance. To address this, we introduce SIFT, a data selection algorithm designed to reduce uncertainty about the model's response given a prompt, which unifies ideas from retrieval and active learning. Whereas Nearest Neighbor retrieval typically fails in the presence of information duplication, SIFT accounts for information duplication and optimizes the overall information gain of the selected examples. We focus our evaluations on fine-tuning at test-time for prompt-specific language modeling on the Pile dataset, and show that SIFT consistently outperforms Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we show that our uncertainty estimates can predict the performance gain of test-time fine-tuning, and use this to develop an adaptive algorithm that invests test-time compute proportional to realized performance gains. We provide the $\texttt{activeft}$ (Active Fine-Tuning) library which can be used as a drop-in replacement for Nearest Neighbor retrieval.
    </details>










































ğŸ”¹ [O1 Replication Journey: A Strategic Progress Report - Part 1](https://arxiv.org/abs/2410.18982)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2410.18982)
- ğŸ‘¤ **Authors:** Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, Pengfei Liu
- ğŸ—“ï¸ **Date:** 2024-10-08
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    This paper introduces a pioneering approach to artificial intelligence research, embodied in our O1 Replication Journey. In response to the announcement of OpenAI's groundbreaking O1 model, we embark on a transparent, real-time exploration to replicate its capabilities while reimagining the process of conducting and communicating AI research. Our methodology addresses critical challenges in modern AI research, including the insularity of prolonged team-based projects, delayed information sharing, and the lack of recognition for diverse contributions. By providing comprehensive, real-time documentation of our replication efforts, including both successes and failures, we aim to foster open science, accelerate collective advancement, and lay the groundwork for AI-driven scientific discovery. Our research progress report diverges significantly from traditional research papers, offering continuous updates, full process transparency, and active community engagement throughout the research journey. Technologically, we proposed the journey learning paradigm, which encourages models to learn not just shortcuts, but the complete exploration process, including trial and error, reflection, and backtracking. With only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8\% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded. We share valuable resources including technical hypotheses and insights, cognitive exploration maps, custom-developed tools, etc at https://github.com/GAIR-NLP/O1-Journey.
    </details>












































































ğŸ”¹ [Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation](https://arxiv.org/abs/2410.02725)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2410.02725)
- ğŸ‘¤ **Authors:** Rohin Manvi, Anikait Singh, Stefano Ermon
- ğŸ—“ï¸ **Date:** 2024-10-03
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval increases from 21% to 34% with 16 samples and math performance on GSM8K improves from 84% to 91%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50-75% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs.
    </details>






















































































































ğŸ”¹ [ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning](https://arxiv.org/abs/2410.02052)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2410.02052)
- ğŸ‘¤ **Authors:** Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu
- ğŸ—“ï¸ **Date:** 2024-10-02
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon tasks. To address these limitations, we present ExACT, an approach to combine test-time search and self-learning to build o1-like models for agentic applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test time algorithm designed to enhance AI agents' ability to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate for reliable state evaluation. Next, we introduce Exploratory Learning, a novel learning strategy to teach agents to search at inference time without relying on any external search algorithms. On the challenging VisualWebArena benchmark, our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge and experience gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1) demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success, and 2) matches 87% of R-MCTS's performance while using significantly less compute. Notably, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' capabilities for agentic applications via test-time search and self-learning.
    </details>















































































































































ğŸ”¹ [RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2409.09584)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2409.09584)
- ğŸ‘¤ **Authors:** Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, Weinan Zhang
- ğŸ—“ï¸ **Date:** 2024-09-15
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    LLM agents enhanced by tree search algorithms have yielded notable performances in code generation. However, current search algorithms in this domain suffer from low search quality due to several reasons: 1) Ineffective design of the search space for the high-reasoning demands of code generation tasks, 2) Inadequate integration of code feedback with the search algorithm, and 3) Poor handling of negative feedback during the search, leading to reduced search efficiency and quality. To address these challenges, we propose to search for the reasoning process of the code and use the detailed feedback of code execution to refine erroneous thoughts during the search. In this paper, we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS) algorithm to conduct thought-level searches before generating code, thereby exploring a wider range of strategies. More importantly, we construct verbal feedback from fine-grained code execution feedback to refine erroneous thoughts during the search. This ensures that the search progresses along the correct reasoning paths, thus improving the overall search quality of the tree by leveraging execution feedback. Through extensive experiments, we demonstrate that RethinkMCTS outperforms previous search-based and feedback-based code generation baselines. On the HumanEval dataset, it improves the pass@1 of GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It effectively conducts more thorough exploration through thought-level searches and enhances the search quality of the entire tree by incorporating rethink operation.
    </details>














































































ğŸ”¹ [Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents](https://arxiv.org/abs/2408.07199)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2408.07199)
- ğŸ‘¤ **Authors:** Pranav Putta, Edmund Mills, Naman Garg, S. Motwani, Chelsea Finn, Divyansh Garg, Rafael Rafailov
- ğŸ—“ï¸ **Date:** 2024-08-13
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.
    </details>
















































































































































ğŸ”¹ [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2408.03314)
- ğŸ‘¤ **Authors:** Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
- ğŸ—“ï¸ **Date:** 2024-08-06
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a"compute-optimal"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.
    </details>

















































































































































ğŸ”¹ [Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models](https://arxiv.org/abs/2408.00724)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2408.00724)
- ğŸ‘¤ **Authors:** Yangzhen Wu, Zhiqing Sun, Shanda Li, S. Welleck, Yiming Yang
- ğŸ—“ï¸ **Date:** 2024-08-01
- ğŸ“‘ **Publisher:** ArXiv
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings indicate smaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets, and that smaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets. We hope these findings contribute to a broader understanding of inference scaling laws for LLMs.
    </details>

















































































































































ğŸ”¹ [Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](https://arxiv.org/abs/2407.21787)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2407.21787)
- ğŸ‘¤ **Authors:** Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R'e, Azalia Mirhoseini
- ğŸ—“ï¸ **Date:** 2024-07-31
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.
    </details>

















































































































































ğŸ”¹ [Palu: KV-Cache Compression with Low-Rank Projection](https://arxiv.org/abs/2407.21118)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2407.21118)
- ğŸ‘¤ **Authors:** Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, N. Huang, Luis Ceze, Mohamed S. Abdelfattah, Kai-Chiang Wu
- ğŸ—“ï¸ **Date:** 2024-07-30
- ğŸ“‘ **Publisher:** International Conference on Learning Representations
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tensors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) optimized GPU kernels with operators fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89x on the RoPE-based attention module. When combined with quantization, Palu's inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91x speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu's superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu
    </details>





































ğŸ”¹ [Tree Search for Language Model Agents](https://arxiv.org/abs/2407.01476)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2407.01476)
- ğŸ‘¤ **Authors:** Jing Yu Koh, Stephen McAleer, Daniel Fried, Ruslan Salakhutdinov
- ğŸ—“ï¸ **Date:** 2024-07-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at https://jykoh.com/search-agents.
    </details>

















































































































































ğŸ”¹ [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/abs/2406.16838)
- ğŸ”— **ArXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2406.16838)
- ğŸ‘¤ **Authors:** S. Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui
- ğŸ—“ï¸ **Date:** 2024-06-24
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.
    </details>









































































































































































ğŸ”¹ [ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search](https://arxiv.org/abs/2406.03816)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2406.03816)
- ğŸ‘¤ **Authors:** Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, Jie Tang
- ğŸ—“ï¸ **Date:** 2024-06-06
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\text{EM}$ and Self-Rewarding LM. We release all code at https://github.com/THUDM/ReST-MCTS.
    </details>


























































































































ğŸ”¹ [Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning](https://arxiv.org/abs/2405.00451)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2405.00451)
- ğŸ‘¤ **Authors:** Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, T. Lillicrap, Kenji Kawaguchi, Michael Shieh
- ğŸ—“ï¸ **Date:** 2024-05-01
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\%$ (+$5.9\%$), $34.7\%$ (+$5.8\%$), and $76.4\%$ (+$15.8\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.
    </details>













































































ğŸ”¹ [V-STaR: Training Verifiers for Self-Taught Reasoners](https://arxiv.org/abs/2402.06457)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2402.06457)
- ğŸ‘¤ **Authors:** Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron C. Courville, Alessandro Sordoni, Rishabh Agarwal
- ğŸ—“ï¸ **Date:** 2024-02-09
- ğŸ“‘ **Publisher:** arXiv.org
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Common self-improvement approaches for large language models (LLMs), such as STaR, iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
    </details>




























































































































ğŸ”¹ [Test-Time Training on Nearest Neighbors for Large Language Models](https://arxiv.org/abs/2305.18466)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2305.18466)
- ğŸ‘¤ **Authors:** Moritz Hardt, Yu Sun
- ğŸ—“ï¸ **Date:** 2023-05-29
- ğŸ“‘ **Publisher:** International Conference on Learning Representations
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.
    </details>











































ğŸ”¹ [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2305.14992)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2305.14992)
- ğŸ‘¤ **Authors:** Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, D. Wang, Zhiting Hu
- ğŸ—“ï¸ **Date:** 2023-05-24
- ğŸ“‘ **Publisher:** Conference on Empirical Methods in Natural Language Processing
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.
    </details>






















































































































































ğŸ”¹ [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2305.10601)
- ğŸ‘¤ **Authors:** Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, T. Griffiths, Yuan Cao, Karthik Narasimhan
- ğŸ—“ï¸ **Date:** 2023-05-17
- ğŸ“‘ **Publisher:** Neural Information Processing Systems
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.
    </details>




































































































































ğŸ”¹ [Self-Evaluation Guided Beam Search for Reasoning](https://arxiv.org/abs/2305.00633)
- ğŸ”— **arXiv PDF Link:** [Paper Link](https://arxiv.org/pdf/2305.00633)
- ğŸ‘¤ **Authors:** Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, MingSung Kan, Junxian He, Qizhe Xie
- ğŸ—“ï¸ **Date:** 2023-05-01
- ğŸ“‘ **Publisher:** Neural Information Processing Systems
- ğŸ“ **Abstract:** 
    <details>
    <summary>Expand</summary>
    Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.
    </details>









































































































































































































































<<<<<<< HEAD











































































































































